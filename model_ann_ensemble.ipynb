{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dense, BatchNormalization, Activation, GlobalAveragePooling1D, Dropout, Normalization, LSTM, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 123\n",
    "tf.random.set_seed(seed_value)\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = r\"C:\\Users\\tmhnguyen\\Documents\\lalamove\\lalamove\\data\\Clean_extracted_240115_uncal\\train\"\n",
    "labels = [5, 6, 7]\n",
    "synthetic_percent = {5: 0.8, 6: 0.1, 7: 0.5}\n",
    "with open(basedir + '/../data_split_params.json', 'r') as file:\n",
    "    features = json.load(file)['FEATURES']\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ori = pd.read_csv(basedir + f'/{label}/train_label_{label}.csv')\n",
    "X_ori = []\n",
    "step = 30_000\n",
    "for i in range(np.ceil(len(y_ori)/30_000).astype(int)):\n",
    "    temp = pd.read_csv(basedir + f'/{label}/extract_features_{label}_{i}.csv', index_col=0)\n",
    "    X_ori.append(temp)\n",
    "X_ori = pd.concat(X_ori)\n",
    "assert len(X_ori) == len(y_ori), f\"Length mismatch {len(X_ori)}, {len(y_ori)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(xtrain, input_shape=500):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    scaler = Normalization()\n",
    "    scaler.adapt(xtrain)\n",
    "    scaled_inputs = scaler(inputs)\n",
    "    # x = Dense(500, activation='relu')(scaled_inputs)\n",
    "    reshape_input = Reshape((5, input_shape//5))(scaled_inputs)\n",
    "    x = LSTM(64)(reshape_input)  # LSTM layer with 64 units\n",
    "    # x = Dense(100, activation='relu')(x)\n",
    "    # x = Dropout(0.3)(x)\n",
    "    # x = Dense(100, activation='relu')(x)\n",
    "    # x = Dropout(0.3)(x)\n",
    "    # x = Dense(100, activation='relu')(x)\n",
    "    # x = Dropout(0.3)(x)\n",
    "    # x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = y_ori.date.unique()\n",
    "test_date = dates[-3]\n",
    "test_idx = y_ori[(y_ori.date == test_date) & (y_ori.type == 0)].index\n",
    "\n",
    "X_test = X_ori.iloc[test_idx]\n",
    "y_test = y_ori.iloc[test_idx].reset_index().label\n",
    "\n",
    "y = y_ori[y_ori.date != test_date].reset_index()\n",
    "X = X_ori.iloc[y.index]\n",
    "\n",
    "print('test date is ', test_date, X_ori.shape, y_ori.shape, X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "alphas = []\n",
    "n_models = len(dates) - 1\n",
    "rand_states = np.random.randint(1, 10000, size=n_models)\n",
    "for i, val_date in enumerate([d for d in dates if d != test_date]):\n",
    "    print(f'{i+1}th model in the ensemble', val_date)\n",
    "    tf.keras.backend.clear_session() # release resource associated with previous model\n",
    "    \n",
    "    val_idx = y[(y.date == val_date) & (y.type == 0)].index\n",
    "    train_idx = y[(y.date != val_date) & (y.type == 0)]\n",
    "    train_idx_add = y[(y.date != val_date) & (y.type == 1)].sample(frac=synthetic_percent[label])\n",
    "    train_idx = pd.concat([train_idx, train_idx_add]).index\n",
    "\n",
    "    X.iloc[val_idx].to_numpy()\n",
    "    X.iloc[train_idx_add.index].to_numpy()\n",
    "\n",
    "    X_train_en, X_val = X.iloc[train_idx].to_numpy(), X.iloc[val_idx].to_numpy()\n",
    "    y_train_en, y_val = y.iloc[train_idx].label.to_numpy(), y.iloc[val_idx].label.to_numpy()\n",
    "    \n",
    "    print(X_train_en.shape, X_val.shape)\n",
    "\n",
    "    model = create_model(X_train_en, input_shape=X_train_en.shape[1])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=2e-5),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=[BinaryAccuracy(name='acc'),\n",
    "                        Precision(name='precision'),\n",
    "                        Recall(name='recall')])\n",
    "\n",
    "    history = model.fit(X_train_en, y_train_en, batch_size=256, epochs=5, validation_data=(X_val, y_val),\n",
    "                        callbacks=[EarlyStopping(patience=20,\n",
    "                                        min_delta=0.0005,\n",
    "                                        restore_best_weights=True)],\n",
    "                        verbose=True)\n",
    "    loss_v, acc_v, precision_v, recall_v = model.evaluate(X_val, y_val)\n",
    "\n",
    "    # Compute the weighted error of the weak classifier\n",
    "    predictions = (model.predict(X_train_en) >= 0.5).flatten().astype(int)\n",
    "    weighted_error = np.sum(predictions != y_train_en) / len(X_train_en)\n",
    "    \n",
    "    # Compute the model weight of the weak classifier\n",
    "    alpha = 0.5 * np.log((1 - weighted_error) / weighted_error)\n",
    "    alphas.append(alpha)\n",
    "\n",
    "    # print(f' on validation set - loss: {loss_v:.4f}, accuracy: {acc_v:.4f}, precision: {precision_v:.4f}, recal: {recall_v:.4f}')\n",
    "    model.save(basedir + f'/../model_ann_ensemble/{label}/model_{label}_no_{i}.hdf5')\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, alphas, xtest):\n",
    "    ensemble_predictions = np.sum([model.predict(xtest)*alpha for model, alpha in zip(models, alphas)], axis=0)\n",
    "    return ensemble_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized model weights\n",
    "alphas = np.array(alphas)\n",
    "alphas = alphas / alphas.sum()\n",
    "dates, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(row, col='pred'):\n",
    "    true, pred = row.true, row[col]\n",
    "    if true == pred and true == 0:\n",
    "        return 'True Negative'\n",
    "    elif true == pred and true == 1:\n",
    "        return 'True Positive'\n",
    "    elif true != pred and true == 0:\n",
    "        return 'False Positive'\n",
    "    else:\n",
    "        return 'False Negative'\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(models), ncols=2, figsize=(10, 25))\n",
    "for k, model in enumerate(models):    \n",
    "    pred = model.predict(X_test) >= 0.5\n",
    "    df = pd.DataFrame(np.hstack((y_test.to_numpy().reshape(-1, 1), pred)), columns=['true', 'pred'])\n",
    "    df.pred = df.pred.astype(int)        \n",
    "    df['type'] = df.apply(lambda x: classify(x), axis=1)\n",
    "    types = df.type.value_counts().sort_index()[::-1]\n",
    "\n",
    "    w = 5 # window in seconds\n",
    "    pred_avg = np.convolve(pred.flatten(), np.ones(w), mode='same') / w >= 0.5\n",
    "    df['pred_avg'] = pred_avg\n",
    "    df.pred_avg = df.pred_avg.astype(int)        \n",
    "    df['type_avg'] = df.apply(lambda x: classify(x, col='pred_avg'), axis=1)\n",
    "    types_avg = df.type_avg.value_counts().sort_index()[::-1]\n",
    "\n",
    "    colors = ['skyblue', 'blue', 'green', 'red']\n",
    "    types_ = ['True Negative', 'True Positive', 'False Negative', 'False Positive']\n",
    "\n",
    "    i = 0.5\n",
    "    for j, t in enumerate(types_):\n",
    "        try:\n",
    "            axes[k, 0].scatter(df[df.type==t].index, [i]*types[t], label=t, c=colors[j])\n",
    "        except KeyError:\n",
    "            pass\n",
    "        i += 0.1\n",
    "\n",
    "    axes[k, 0].set_ylim(0, 2)\n",
    "    axes[k, 0].get_yaxis().set_visible(False)\n",
    "    axes[k, 0].set_title(f'Label {label} - test_date {dates[k]}')\n",
    "\n",
    "    i = 0.5\n",
    "    for j, t in enumerate(types_):\n",
    "        try:\n",
    "            axes[k, 1].scatter(df[df.type_avg==t].index, [i]*types_avg[t], label=t, c=colors[j])\n",
    "        except KeyError:\n",
    "            pass\n",
    "        i += 0.1\n",
    "\n",
    "    axes[k, 1].set_ylim(0, 2)\n",
    "    axes[k, 1].get_yaxis().set_visible(False)\n",
    "    axes[k, 1].set_title(f'Label {label} - test_date {dates[k]}')  \n",
    "\n",
    "axes[0, 0].legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ensemble_predict(models, alphas, X_test) >= 0.5\n",
    "print(pred.shape, y_test.shape)\n",
    "\n",
    "df = pd.DataFrame(np.hstack((y_test.to_numpy().reshape(-1, 1), pred)), columns=['true', 'pred'])\n",
    "df.pred = df.pred.astype(int)\n",
    "\n",
    "def classify(row):\n",
    "    true, pred = row.true, row.pred\n",
    "    if true == pred and true == 0:\n",
    "        return 'True Negative'\n",
    "    elif true == pred and true == 1:\n",
    "        return 'True Positive'\n",
    "    elif true != pred and true == 0:\n",
    "        return 'False Positive'\n",
    "    else:\n",
    "        return 'False Negative'\n",
    "    \n",
    "df['type'] = df.apply(lambda x: classify(x), axis=1)\n",
    "types = df.type.value_counts().sort_index()[::-1]\n",
    "print(types)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 2.5))\n",
    "i = 0\n",
    "colors = ['skyblue', 'blue', 'green', 'red']\n",
    "types_ = ['True Negative', 'True Positive', 'False Negative', 'False Positive']\n",
    "\n",
    "for j, t in enumerate(types_):\n",
    "    try:\n",
    "        ax.scatter(df[df.type==t].index, [i]*types[t], label=t, c=colors[j])\n",
    "    except KeyError:\n",
    "        pass\n",
    "    i += 0.1\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 2)\n",
    "ax.set_xlabel('Seconds')\n",
    "ax.get_yaxis().set_visible(False)\n",
    "ax.set_title(f'{label} NO average smoothing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ensemble_predict(models, alphas, X_test).flatten() \n",
    "print(pred.shape, y_test.shape)\n",
    "w = 5 # window in seconds\n",
    "pred = np.convolve(pred, np.ones(w), mode='same') / w >= 0.5\n",
    "\n",
    "print(pred.shape, y_test.shape)\n",
    "df = pd.DataFrame(np.stack((y_test, pred)).T, columns=['true', 'pred'])\n",
    "df.pred = df.pred.astype(int)\n",
    "\n",
    "def classify(row):\n",
    "    true, pred = row.true, row.pred\n",
    "    if true == pred and true == 0:\n",
    "        return 'True Negative'\n",
    "    elif true == pred and true == 1:\n",
    "        return 'True Positive'\n",
    "    elif true != pred and true == 0:\n",
    "        return 'False Positive'\n",
    "    else:\n",
    "        return 'False Negative'\n",
    "    \n",
    "df['type'] = df.apply(lambda x: classify(x), axis=1)\n",
    "types = df.type.value_counts().sort_index()[::-1]\n",
    "print(types)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 2.5))\n",
    "i = 0\n",
    "colors = ['skyblue', 'blue', 'green', 'red']\n",
    "types_ = ['True Negative', 'True Positive', 'False Negative', 'False Positive']\n",
    "\n",
    "for j, t in enumerate(types_):\n",
    "    try:\n",
    "        ax.scatter(df[df.type==t].index, [i]*types[t], label=t, c=colors[j])\n",
    "    except KeyError:\n",
    "        print(f'There is no {t}')\n",
    "    i += 0.1\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 2)\n",
    "ax.set_xlabel('Seconds')\n",
    "ax.get_yaxis().set_visible(False)\n",
    "ax.set_title(f'{label} with average smoothing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsfresh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
